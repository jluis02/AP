{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of videos with frames: 30\n"
     ]
    }
   ],
   "source": [
    "LABEL_PATH = Path(\"../data/labels/labels_task2.csv\")\n",
    "FRAME_DIR = Path(\"../data/frames\")\n",
    "\n",
    "df = pd.read_csv(LABEL_PATH)\n",
    "available_videos = {p.name for p in FRAME_DIR.iterdir() if p.is_dir() and any(p.glob(\"*.jpg\"))}\n",
    "df = df[df[\"VIDEO\"].isin(available_videos)].reset_index(drop=True)\n",
    "print(f\"Number of videos with frames: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OSATSDataset(Dataset):\n",
    "    def __init__(self, dataframe, frame_dir, transform=None, sequence_length=16):\n",
    "        self.data = dataframe.copy()\n",
    "        self.frame_dir = frame_dir\n",
    "        self.transform = transform\n",
    "        self.sequence_length = sequence_length\n",
    "        self.osats_cols = [col for col in dataframe.columns if col.startswith(\"OSATS_\")]\n",
    "\n",
    "        for col in self.osats_cols:\n",
    "            self.data[col] = self.data[col].clip(0, 4).astype(np.int64)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        video_id = row[\"VIDEO\"]\n",
    "        y = row[self.osats_cols].values.astype(np.int64)\n",
    "        path = self.frame_dir / video_id\n",
    "\n",
    "        frames = sorted(path.glob(\"*.jpg\"))\n",
    "        selected = frames[:self.sequence_length]\n",
    "        if len(selected) == 0:\n",
    "            raise IndexError(f\"No frames for video {video_id}\")\n",
    "        while len(selected) < self.sequence_length:\n",
    "            selected.append(selected[-1])\n",
    "\n",
    "        images = [self.transform(Image.open(f).convert(\"RGB\")) for f in selected]\n",
    "        return torch.stack(images), torch.tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "dataset = OSATSDataset(df, FRAME_DIR, transform)\n",
    "loader = DataLoader(dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.init import kaiming_uniform_, xavier_uniform_\n",
    "\n",
    "class CNNModel_1(nn.Module):\n",
    "    def __init__(self, num_classes=40, sequence_length=16, input_shape=(3,224,224)):\n",
    "        super(CNNModel_1, self).__init__()\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 32, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "\n",
    "        conv_out_size = self._get_conv_output((sequence_length, *input_shape))\n",
    "        self.fc1 = nn.Linear(conv_out_size, 100)\n",
    "        kaiming_uniform_(self.fc1.weight, nonlinearity='relu')\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(100, num_classes)\n",
    "        xavier_uniform_(self.fc2.weight)\n",
    "        self.act2 = nn.Softmax(dim=1)\n",
    "\n",
    "    def _get_conv_output(self, shape):\n",
    "        bs = 1\n",
    "        input = torch.rand(bs, *shape)\n",
    "        B, T, C, H, W = input.shape\n",
    "        input = input.view(B * T, C, H, W)\n",
    "        output_feat = self.layer1(input)\n",
    "        output_feat = self.layer2(output_feat)\n",
    "        output_feat = output_feat.view(output_feat.size(0), -1)\n",
    "        output_feat = output_feat.view(bs, T, -1).mean(dim=1)\n",
    "        return int(np.prod(output_feat.size()[1:]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C, H, W = x.shape\n",
    "        x = x.view(B * T, C, H, W)\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = out.view(B, T, -1).mean(dim=1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.act1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.act2(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel_2(nn.Module):\n",
    "    def __init__(self, num_classes=40, sequence_length=16, input_shape=(3,224,224)):\n",
    "        super(CNNModel_2, self).__init__()\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        conv_out_size = self._get_conv_output((sequence_length, *input_shape))\n",
    "        self.fc1 = nn.Linear(conv_out_size, num_classes)\n",
    "\n",
    "    def _get_conv_output(self, shape):\n",
    "        bs = 1\n",
    "        input = torch.rand(bs, *shape)\n",
    "        B, T, C, H, W = input.shape\n",
    "        input = input.view(B * T, C, H, W)\n",
    "        output_feat = self.layer1(input)\n",
    "        output_feat = self.layer2(output_feat)\n",
    "        output_feat = output_feat.view(output_feat.size(0), -1)\n",
    "        output_feat = output_feat.view(bs, T, -1).mean(dim=1)\n",
    "        return int(np.prod(output_feat.size()[1:]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C, H, W = x.shape\n",
    "        x = x.view(B * T, C, H, W)\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = out.view(B, T, -1).mean(dim=1)\n",
    "        out = self.fc1(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import BatchNorm2d, Dropout\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNNModel_3(nn.Module):\n",
    "    def __init__(self, num_classes=40, sequence_length=16, input_shape=(3,224,224)):\n",
    "        super(CNNModel_3, self).__init__()\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1),\n",
    "            BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, 3),\n",
    "            BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        conv_out_size = self._get_conv_output((sequence_length, *input_shape))\n",
    "        self.fc1 = nn.Linear(conv_out_size, 600)\n",
    "        self.drop = Dropout(0.25)\n",
    "        self.fc2 = nn.Linear(600, 120)\n",
    "        self.fc3 = nn.Linear(120, num_classes)\n",
    "\n",
    "    def _get_conv_output(self, shape):\n",
    "        bs = 1\n",
    "        input = torch.rand(bs, *shape)\n",
    "        B, T, C, H, W = input.shape\n",
    "        input = input.view(B * T, C, H, W)\n",
    "        output_feat = self.layer1(input)\n",
    "        output_feat = self.layer2(output_feat)\n",
    "        output_feat = output_feat.view(output_feat.size(0), -1)\n",
    "        output_feat = output_feat.view(bs, T, -1).mean(dim=1)\n",
    "        return int(np.prod(output_feat.size()[1:]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C, H, W = x.shape\n",
    "        x = x.view(B * T, C, H, W)\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = out.view(B, T, -1).mean(dim=1)\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = self.drop(out)\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = self.fc3(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Dropout2d\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNNModel_4(nn.Module):\n",
    "    def __init__(self, num_classes=40, sequence_length=16, input_shape=(3,224,224)):\n",
    "        super(CNNModel_4, self).__init__()\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 5),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            Dropout2d(0.2)\n",
    "        )\n",
    "\n",
    "        conv_out_size = self._get_conv_output((sequence_length, *input_shape))\n",
    "        self.fc1 = nn.Linear(conv_out_size, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def _get_conv_output(self, shape):\n",
    "        bs = 1\n",
    "        input = torch.rand(bs, *shape)\n",
    "        B, T, C, H, W = input.shape\n",
    "        input = input.view(B * T, C, H, W)\n",
    "        output_feat = self.layer1(input)\n",
    "        output_feat = output_feat.view(output_feat.size(0), -1)\n",
    "        output_feat = output_feat.view(bs, T, -1).mean(dim=1)\n",
    "        return int(np.prod(output_feat.size()[1:]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C, H, W = x.shape\n",
    "        x = x.view(B * T, C, H, W)\n",
    "        out = self.layer1(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = out.view(B, T, -1).mean(dim=1)\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, epochs):\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            outputs = model(inputs)  # [batch_size, num_classes]\n",
    "\n",
    "            if targets.dim() > 1:\n",
    "                targets = targets[:, 0]  # assume mesma label para toda a sequência\n",
    "\n",
    "            if outputs.shape[0] != targets.shape[0]:\n",
    "                raise ValueError(f\"Shape mismatch: outputs {outputs.shape} vs targets {targets.shape}\")\n",
    "\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(dataloader):.4f}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 3.5715\n",
      "Epoch [2/100], Loss: 3.4184\n",
      "Epoch [3/100], Loss: 3.4497\n",
      "Epoch [4/100], Loss: 3.4184\n",
      "Epoch [5/100], Loss: 3.4184\n",
      "Epoch [6/100], Loss: 3.4497\n",
      "Epoch [7/100], Loss: 3.4184\n",
      "Epoch [8/100], Loss: 3.4184\n",
      "Epoch [9/100], Loss: 3.4497\n",
      "Epoch [10/100], Loss: 3.3872\n",
      "Epoch [11/100], Loss: 3.4184\n",
      "Epoch [12/100], Loss: 3.4497\n",
      "Epoch [13/100], Loss: 3.4497\n",
      "Epoch [14/100], Loss: 3.4184\n",
      "Epoch [15/100], Loss: 3.3872\n",
      "Epoch [16/100], Loss: 3.4184\n",
      "Epoch [17/100], Loss: 3.3872\n",
      "Epoch [18/100], Loss: 3.3872\n",
      "Epoch [19/100], Loss: 3.4497\n",
      "Epoch [20/100], Loss: 3.4497\n",
      "Epoch [21/100], Loss: 3.3872\n",
      "Epoch [22/100], Loss: 3.4497\n",
      "Epoch [23/100], Loss: 3.4184\n",
      "Epoch [24/100], Loss: 3.4184\n",
      "Epoch [25/100], Loss: 3.3872\n",
      "Epoch [26/100], Loss: 3.4497\n",
      "Epoch [27/100], Loss: 3.4497\n",
      "Epoch [28/100], Loss: 3.4184\n",
      "Epoch [29/100], Loss: 3.4497\n",
      "Epoch [30/100], Loss: 3.4497\n",
      "Epoch [31/100], Loss: 3.4497\n",
      "Epoch [32/100], Loss: 3.4497\n",
      "Epoch [33/100], Loss: 3.4497\n",
      "Epoch [34/100], Loss: 3.4497\n",
      "Epoch [35/100], Loss: 3.4497\n",
      "Epoch [36/100], Loss: 3.3872\n",
      "Epoch [37/100], Loss: 3.4497\n",
      "Epoch [38/100], Loss: 3.3872\n",
      "Epoch [39/100], Loss: 3.4184\n",
      "Epoch [40/100], Loss: 3.4184\n",
      "Epoch [41/100], Loss: 3.4497\n",
      "Epoch [42/100], Loss: 3.4497\n",
      "Epoch [43/100], Loss: 3.4497\n",
      "Epoch [44/100], Loss: 3.4184\n",
      "Epoch [45/100], Loss: 3.4184\n",
      "Epoch [46/100], Loss: 3.4184\n",
      "Epoch [47/100], Loss: 3.4184\n",
      "Epoch [48/100], Loss: 3.4184\n",
      "Epoch [49/100], Loss: 3.4184\n",
      "Epoch [50/100], Loss: 3.4497\n",
      "Epoch [51/100], Loss: 3.4497\n",
      "Epoch [52/100], Loss: 3.4497\n",
      "Epoch [53/100], Loss: 3.4497\n",
      "Epoch [54/100], Loss: 3.4497\n",
      "Epoch [55/100], Loss: 3.4497\n",
      "Epoch [56/100], Loss: 3.4184\n",
      "Epoch [57/100], Loss: 3.4497\n",
      "Epoch [58/100], Loss: 3.4497\n",
      "Epoch [59/100], Loss: 3.4184\n",
      "Epoch [60/100], Loss: 3.4184\n",
      "Epoch [61/100], Loss: 3.4497\n",
      "Epoch [62/100], Loss: 3.4497\n",
      "Epoch [63/100], Loss: 3.4184\n",
      "Epoch [64/100], Loss: 3.4497\n",
      "Epoch [65/100], Loss: 3.4497\n",
      "Epoch [66/100], Loss: 3.4497\n",
      "Epoch [67/100], Loss: 3.4184\n",
      "Epoch [68/100], Loss: 3.4497\n",
      "Epoch [69/100], Loss: 3.3872\n",
      "Epoch [70/100], Loss: 3.4497\n",
      "Epoch [71/100], Loss: 3.4184\n",
      "Epoch [72/100], Loss: 3.4184\n",
      "Epoch [73/100], Loss: 3.4497\n",
      "Epoch [74/100], Loss: 3.4497\n",
      "Epoch [75/100], Loss: 3.4184\n",
      "Epoch [76/100], Loss: 3.4184\n",
      "Epoch [77/100], Loss: 3.4184\n",
      "Epoch [78/100], Loss: 3.4497\n",
      "Epoch [79/100], Loss: 3.4497\n",
      "Epoch [80/100], Loss: 3.4497\n",
      "Epoch [81/100], Loss: 3.4184\n",
      "Epoch [82/100], Loss: 3.4184\n",
      "Epoch [83/100], Loss: 3.4497\n",
      "Epoch [84/100], Loss: 3.4497\n",
      "Epoch [85/100], Loss: 3.4184\n",
      "Epoch [86/100], Loss: 3.4497\n",
      "Epoch [87/100], Loss: 3.4497\n",
      "Epoch [88/100], Loss: 3.4184\n",
      "Epoch [89/100], Loss: 3.4184\n",
      "Epoch [90/100], Loss: 3.4184\n",
      "Epoch [91/100], Loss: 3.4497\n",
      "Epoch [92/100], Loss: 3.4497\n",
      "Epoch [93/100], Loss: 3.4184\n",
      "Epoch [94/100], Loss: 3.4184\n",
      "Epoch [95/100], Loss: 3.4184\n",
      "Epoch [96/100], Loss: 3.3872\n",
      "Epoch [97/100], Loss: 3.4497\n",
      "Epoch [98/100], Loss: 3.4497\n",
      "Epoch [99/100], Loss: 3.4497\n",
      "Epoch [100/100], Loss: 3.4497\n",
      "Epoch [1/100], Loss: 1.9672\n",
      "Epoch [2/100], Loss: 1.2408\n",
      "Epoch [3/100], Loss: 1.1076\n",
      "Epoch [4/100], Loss: 0.7025\n",
      "Epoch [5/100], Loss: 0.6071\n",
      "Epoch [6/100], Loss: 0.4823\n",
      "Epoch [7/100], Loss: 0.3876\n",
      "Epoch [8/100], Loss: 0.3037\n",
      "Epoch [9/100], Loss: 0.2370\n",
      "Epoch [10/100], Loss: 0.1854\n",
      "Epoch [11/100], Loss: 0.1307\n",
      "Epoch [12/100], Loss: 0.0929\n",
      "Epoch [13/100], Loss: 0.0776\n",
      "Epoch [14/100], Loss: 0.0640\n",
      "Epoch [15/100], Loss: 0.0469\n",
      "Epoch [16/100], Loss: 0.0417\n",
      "Epoch [17/100], Loss: 0.0320\n",
      "Epoch [18/100], Loss: 0.0282\n",
      "Epoch [19/100], Loss: 0.0272\n",
      "Epoch [20/100], Loss: 0.0234\n",
      "Epoch [21/100], Loss: 0.0198\n",
      "Epoch [22/100], Loss: 0.0170\n",
      "Epoch [23/100], Loss: 0.0171\n",
      "Epoch [24/100], Loss: 0.0175\n",
      "Epoch [25/100], Loss: 0.0128\n",
      "Epoch [26/100], Loss: 0.0139\n",
      "Epoch [27/100], Loss: 0.0113\n",
      "Epoch [28/100], Loss: 0.0104\n",
      "Epoch [29/100], Loss: 0.0095\n",
      "Epoch [30/100], Loss: 0.0083\n",
      "Epoch [31/100], Loss: 0.0081\n",
      "Epoch [32/100], Loss: 0.0078\n",
      "Epoch [33/100], Loss: 0.0074\n",
      "Epoch [34/100], Loss: 0.0064\n",
      "Epoch [35/100], Loss: 0.0062\n",
      "Epoch [36/100], Loss: 0.0059\n",
      "Epoch [37/100], Loss: 0.0055\n",
      "Epoch [38/100], Loss: 0.0051\n",
      "Epoch [39/100], Loss: 0.0046\n",
      "Epoch [40/100], Loss: 0.0047\n",
      "Epoch [41/100], Loss: 0.0043\n",
      "Epoch [42/100], Loss: 0.0041\n",
      "Epoch [43/100], Loss: 0.0040\n",
      "Epoch [44/100], Loss: 0.0038\n",
      "Epoch [45/100], Loss: 0.0036\n",
      "Epoch [46/100], Loss: 0.0034\n",
      "Epoch [47/100], Loss: 0.0033\n",
      "Epoch [48/100], Loss: 0.0031\n",
      "Epoch [49/100], Loss: 0.0030\n",
      "Epoch [50/100], Loss: 0.0029\n",
      "Epoch [51/100], Loss: 0.0030\n",
      "Epoch [52/100], Loss: 0.0028\n",
      "Epoch [53/100], Loss: 0.0027\n",
      "Epoch [54/100], Loss: 0.0024\n",
      "Epoch [55/100], Loss: 0.0023\n",
      "Epoch [56/100], Loss: 0.0023\n",
      "Epoch [57/100], Loss: 0.0021\n",
      "Epoch [58/100], Loss: 0.0020\n",
      "Epoch [59/100], Loss: 0.0021\n",
      "Epoch [60/100], Loss: 0.0021\n",
      "Epoch [61/100], Loss: 0.0018\n",
      "Epoch [62/100], Loss: 0.0019\n",
      "Epoch [63/100], Loss: 0.0018\n",
      "Epoch [64/100], Loss: 0.0017\n",
      "Epoch [65/100], Loss: 0.0017\n",
      "Epoch [66/100], Loss: 0.0017\n",
      "Epoch [67/100], Loss: 0.0015\n",
      "Epoch [68/100], Loss: 0.0015\n",
      "Epoch [69/100], Loss: 0.0015\n",
      "Epoch [70/100], Loss: 0.0015\n",
      "Epoch [71/100], Loss: 0.0014\n",
      "Epoch [72/100], Loss: 0.0013\n",
      "Epoch [73/100], Loss: 0.0013\n",
      "Epoch [74/100], Loss: 0.0013\n",
      "Epoch [75/100], Loss: 0.0012\n",
      "Epoch [76/100], Loss: 0.0012\n",
      "Epoch [77/100], Loss: 0.0012\n",
      "Epoch [78/100], Loss: 0.0011\n",
      "Epoch [79/100], Loss: 0.0011\n",
      "Epoch [80/100], Loss: 0.0011\n",
      "Epoch [81/100], Loss: 0.0011\n",
      "Epoch [82/100], Loss: 0.0010\n",
      "Epoch [83/100], Loss: 0.0010\n",
      "Epoch [84/100], Loss: 0.0010\n",
      "Epoch [85/100], Loss: 0.0009\n",
      "Epoch [86/100], Loss: 0.0010\n",
      "Epoch [87/100], Loss: 0.0009\n",
      "Epoch [88/100], Loss: 0.0009\n",
      "Epoch [89/100], Loss: 0.0009\n",
      "Epoch [90/100], Loss: 0.0008\n",
      "Epoch [91/100], Loss: 0.0008\n",
      "Epoch [92/100], Loss: 0.0008\n",
      "Epoch [93/100], Loss: 0.0008\n",
      "Epoch [94/100], Loss: 0.0008\n",
      "Epoch [95/100], Loss: 0.0008\n",
      "Epoch [96/100], Loss: 0.0007\n",
      "Epoch [97/100], Loss: 0.0007\n",
      "Epoch [98/100], Loss: 0.0007\n",
      "Epoch [99/100], Loss: 0.0007\n",
      "Epoch [100/100], Loss: 0.0007\n",
      "Epoch [1/100], Loss: 3.5760\n",
      "Epoch [2/100], Loss: 1.6804\n",
      "Epoch [3/100], Loss: 1.6725\n",
      "Epoch [4/100], Loss: 0.6924\n",
      "Epoch [5/100], Loss: 0.3989\n",
      "Epoch [6/100], Loss: 0.3129\n",
      "Epoch [7/100], Loss: 0.0512\n",
      "Epoch [8/100], Loss: 0.2632\n",
      "Epoch [9/100], Loss: 0.0430\n",
      "Epoch [10/100], Loss: 0.0626\n",
      "Epoch [11/100], Loss: 0.0497\n",
      "Epoch [12/100], Loss: 0.1111\n",
      "Epoch [13/100], Loss: 0.0026\n",
      "Epoch [14/100], Loss: 0.0051\n",
      "Epoch [15/100], Loss: 0.0199\n",
      "Epoch [16/100], Loss: 0.0209\n",
      "Epoch [17/100], Loss: 0.0027\n",
      "Epoch [18/100], Loss: 0.0174\n",
      "Epoch [19/100], Loss: 0.0011\n",
      "Epoch [20/100], Loss: 0.0015\n",
      "Epoch [21/100], Loss: 0.0105\n"
     ]
    }
   ],
   "source": [
    "model1 = CNNModel_1(num_classes=40, sequence_length=16, input_shape=(3,224,224))\n",
    "hist1 = train_model(model1, loader, 100)\n",
    "\n",
    "model2 = CNNModel_2(num_classes=40, sequence_length=16, input_shape=(3,224,224))\n",
    "hist2 = train_model(model2, loader, 100)\n",
    "\n",
    "model3 = CNNModel_3(num_classes=40, sequence_length=16, input_shape=(3,224,224))\n",
    "hist3 = train_model(model3, loader, 100)\n",
    "\n",
    "model4 = CNNModel_4(num_classes=40, sequence_length=16, input_shape=(3,224,224))\n",
    "hist4 = train_model(model4, loader, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist1, label='CNN Model 1')\n",
    "plt.plot(hist2, label='CNN Model 2')\n",
    "plt.plot(hist3, label='CNN Model 3')\n",
    "plt.plot(hist4, label='CNN Model 4')\n",
    "plt.title('Loss durante treino')\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model1.state_dict(), \"../outputs/models/osats_cnn_model1.pt\")\n",
    "print(\"Modelo 1 salvo com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet18 + MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrigindo escala de labels de 1–5 para 0–4...\n"
     ]
    }
   ],
   "source": [
    "frame_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "def load_video_frames(video_path, max_frames=16):\n",
    "    frames = sorted(video_path.glob(\"*.jpg\"))[:max_frames]\n",
    "    video_tensor = torch.stack([frame_transform(Image.open(f).convert(\"RGB\")) for f in frames])\n",
    "    if len(frames) < max_frames:\n",
    "        padding = torch.zeros((max_frames - len(frames), 3, 224, 224))\n",
    "        video_tensor = torch.cat([video_tensor, padding], dim=0)\n",
    "    return video_tensor\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    video_id = row[\"VIDEO\"]\n",
    "    video_dir = FRAME_DIR / video_id\n",
    "    if not video_dir.exists():\n",
    "        continue\n",
    "    video_tensor = load_video_frames(video_dir, max_frames=16)\n",
    "    labels = torch.tensor([\n",
    "        row[\"OSATS_RESPECT\"], row[\"OSATS_MOTION\"], row[\"OSATS_INSTRUMENT\"],\n",
    "        row[\"OSATS_SUTURE\"], row[\"OSATS_FLOW\"], row[\"OSATS_KNOWLEDGE\"],\n",
    "        row[\"OSATS_PERFORMANCE\"], row[\"OSATS_FINAL_QUALITY\"]\n",
    "    ], dtype=torch.long)\n",
    "    X.append(video_tensor)\n",
    "    y.append(labels)\n",
    "\n",
    "X = torch.stack(X)\n",
    "y = torch.stack(y)\n",
    "\n",
    "# Corrigir labels se estiverem na escala 1–5 (passar para 0–4)\n",
    "if torch.any(y > 4):\n",
    "    print(\"Corrigindo escala de labels de 1–5 para 0–4...\")\n",
    "    y = y - 1\n",
    "\n",
    "# Verificação de segurança\n",
    "assert torch.all((y >= 0) & (y <= 4)), \"Erro: targets fora do intervalo 0–4\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "class OSATSDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "dataset = OSATSDataset(X, y)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OSATSResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        resnet = models.resnet18(pretrained=True)\n",
    "        resnet.fc = nn.Identity()\n",
    "        self.backbone = resnet\n",
    "        self.fc_shared = nn.Linear(512, 128)\n",
    "        self.heads = nn.ModuleList([nn.Linear(128, 5) for _ in range(8)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C, H, W = x.shape\n",
    "        x = x.view(B * T, C, H, W)\n",
    "        feat = self.backbone(x).view(B, T, -1)\n",
    "        feat = feat.mean(dim=1)\n",
    "        shared = F.relu(self.fc_shared(feat))\n",
    "        return torch.stack([head(shared) for head in self.heads], dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def compute_loss(preds, targets):\n",
    "    loss = 0\n",
    "    for i in range(8):\n",
    "        loss += loss_fn(preds[:, i], targets[:, i])\n",
    "    return loss / 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luis/miniconda3/envs/ap/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/luis/miniconda3/envs/ap/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Época 1/20 - Loss treino: 1.6470\n",
      "Época 2/20 - Loss treino: 1.3331\n",
      "Época 3/20 - Loss treino: 1.1281\n",
      "Época 4/20 - Loss treino: 0.9567\n",
      "Época 5/20 - Loss treino: 0.8140\n",
      "Época 6/20 - Loss treino: 0.6887\n",
      "Época 7/20 - Loss treino: 0.5815\n",
      "Época 8/20 - Loss treino: 0.4888\n",
      "Época 9/20 - Loss treino: 0.4093\n",
      "Época 10/20 - Loss treino: 0.3413\n",
      "Época 11/20 - Loss treino: 0.2832\n",
      "Época 12/20 - Loss treino: 0.2338\n",
      "Época 13/20 - Loss treino: 0.1925\n",
      "Época 14/20 - Loss treino: 0.1584\n",
      "Época 15/20 - Loss treino: 0.1303\n",
      "Época 16/20 - Loss treino: 0.1075\n",
      "Época 17/20 - Loss treino: 0.0892\n",
      "Época 18/20 - Loss treino: 0.0744\n",
      "Época 19/20 - Loss treino: 0.0624\n",
      "Época 20/20 - Loss treino: 0.0527\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = OSATSResNet().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    all_preds, all_targets = [], []\n",
    "\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = compute_loss(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "        preds = torch.argmax(outputs, dim=2)\n",
    "        all_preds.append(preds.cpu())\n",
    "        all_targets.append(targets.cpu())\n",
    "\n",
    "    avg_loss = train_loss / len(train_loader.dataset)\n",
    "    print(f\"Época {epoch+1}/{num_epochs} - Loss treino: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds = torch.cat(all_preds)\n",
    "all_targets = torch.cat(all_targets)\n",
    "\n",
    "for i in range(8):\n",
    "    print(f\"\\n--- Critério OSATS {i+1} ---\")\n",
    "    print(classification_report(all_targets[:, i], all_preds[:, i], digits=3))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
